{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EmotionRecognition.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOKbGQ9ax2IbgDAt0HK3IxH"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jVyy6DsV-XgK"},"source":["**Emotion Recognition Model**\n","**Training and Testing**\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"D1OMZ-XY-y1G"},"source":["All these imports are needed, so please install all these libraries\n","\n","**Note:** Please change all the directories as per your system"]},{"cell_type":"code","metadata":{"id":"cAOvkafv5E7a"},"source":["import tensorflow as tf\n","import cv2\n","import os\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import random\n","import pickle\n","from tensorflow import keras\n","from tensorflow.keras import layers"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jQsIRclY_Am7"},"source":["These are the sex classes of emotions, each number representing one emotion:\n","\n","\n","1.   '0' represents 'angry' \n","2.   '1' represents 'disgust'\n","3.   '2' represents 'fear'\n","4.   '3' represents 'happy'\n","5.   '4' represents 'neutral'\n","6.   '5' represents 'sad'\n","7.   '6' represents 'surprised'\n","\n","The folders containing the testing and training images, should have subfolders numbered as above. For example, all 'angry' face images should present in folder names '0'.\n","\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"qaOTmtmp51Bm"},"source":["Classes = [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\"]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x-Xi5j6QAwQR"},"source":["MobileNetv2 requires image size of 224*224\n"]},{"cell_type":"code","metadata":{"id":"UxsP7s8gAvIj"},"source":["img_size=224"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuxcJSlzAA0a"},"source":["This function is needed to create testing and training data from the images"]},{"cell_type":"code","metadata":{"id":"4wsx4_QN5_dN"},"source":["\n","def create_data(data_directory):\n","    data = []\n","    count = 0\n","    file_no = 1\n","    for category in Classes:\n","        path = os.path.join(data_directory, category)\n","        class_num = Classes.index(category)\n","        for img in  os.listdir(path):\n","            count+=1\n","            print(count)\n","            try:\n","                img_array = cv2.imread(os.path.join(path, img))\n","                new_array = cv2.resize(img_array, (img_size, img_size)) ##Bilinear interpolation\n","                data.append([new_array, class_num])\n","                \n","            except Exception as e:\n","                pass\n","\n","    return data"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"64a7MW7bBiUw"},"source":["This part creates the training data which includes the image preprocessing"]},{"cell_type":"code","metadata":{"id":"n1aaUQFC7Vmh"},"source":["data_directory = \"/content/drive/MyDrive/archive/Training\"\n","training_data = create_data(data_directory)\n","random.shuffle(training_Data)\n","X = []\n","y = []\n","for features, label in training_data:\n","    X.append(features)\n","    y.append(label)\n","X = np.array(X).reshape(-1, img_size, img_size, 3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LNEm-KIYBwAS"},"source":["This is to save the dataset as the files can get big"]},{"cell_type":"code","metadata":{"id":"fyIvFdBV9Nw-"},"source":["np.save('/content/drive/MyDrive/archive/train_X.npy', X)\n","np.save('/content/drive/MyDrive/archive/train_y.npy', y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_pk1PInqB5DH"},"source":["This part downloads the pretrained MobileNetV2 model. More layers are added on this model so that it can detect our seven classes of emotions. For loss funtion \"sparse_categorical_crossentropy\" is used and  \"adam\" is used as optimizer."]},{"cell_type":"code","metadata":{"id":"C1_qnm_58iin"},"source":["model = tf.keras.applications.MobileNetV2() \n","final_output = layers.Dense(128)(base_output)\n","final_output = layers.Activation('relu')(final_output)\n","final_output = layers.Dense(64)(final_output)\n","final_output = layers.Activation('relu')(final_output)\n","final_output = layers.Dense(7, activation='softmax')(final_output)\n","new_model = keras.Model(inputs = base_input, outputs = final_output)\n","new_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics = [\"accuracy\"])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qlr7vXpfCYwl"},"source":["This  part is used to train the model"]},{"cell_type":"code","metadata":{"id":"pDqTjFMP6och"},"source":["new_model.fit(np.load('/content/drive/MyDrive/archive/train_X.npy'), np.load('/content/drive/MyDrive/archive/train_y.npy'), batch_size = 1, epochs = 10)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D5R9trF7CdBt"},"source":["This is used to save the trained model in '.h5' format."]},{"cell_type":"code","metadata":{"id":"OAeSkg8Y9SJ5"},"source":["new_model.save('<path to save model>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RIWpX0shClL5"},"source":["This part loads the saved model"]},{"cell_type":"code","metadata":{"id":"msHN_ZkJ9dbF"},"source":["new_model = tf.keras.models.load_model('<saved model path>')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h-mxiGR1CzQC"},"source":["This part is used to generate the testing dataset"]},{"cell_type":"code","metadata":{"id":"5PAag06o9mkV"},"source":["data_directory = \"/content/drive/MyDrive/archive/Testing\"\n","testing_data = create_data(data_directory)\n","X = []\n","y = []\n","for features, label in training_data:\n","    X.append(features)\n","    y.append(label)\n","X = np.array(X).reshape(-1, img_size, img_size, 3)\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hq93KHyKC5CV"},"source":["This parts does testing on the training data set."]},{"cell_type":"code","metadata":{"id":"YyAhEHvr-Sgm"},"source":["correct_map = [0, 0, 0, 0, 0, 0, 0]\n","wrong_map = [0, 0, 0, 0, 0, 0, 0]\n","\n","count = 0\n","for img_index in range(len(X)):\n","  Predictions = new_model.predict(np.expand_dims(X[img_index], axis=0))\n","  count +=1\n","  print(count)\n","  if np.argmax(Predictions) == 0:\n","    if y[img_index] == 0:\n","      correct_map[0] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1\n","\n","  elif np.argmax(Predictions) == 1:\n","    if y[img_index] == 1:\n","      correct_map[1] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1    \n","        \n","  elif np.argmax(Predictions) == 2:\n","    if y[img_index] == 2:\n","      correct_map[2] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1\n","        \n","        \n","  elif np.argmax(Predictions) == 3:\n","    if y[img_index] == 3:\n","      correct_map[3] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1\n","        \n","        \n","  elif np.argmax(Predictions) == 4:\n","    if y[img_index] == 4:\n","      correct_map[4] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1\n","        \n","    \n","  elif np.argmax(Predictions) == 5:\n","    if y[img_index] == 5:\n","      correct_map[5] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1\n","        \n","    \n","  else :\n","    if y[img_index] == 6:\n","      correct_map[6] += 1\n","    else:\n","      wrong_map[y[img_index]] += 1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pkx8hnv0-Vvt"},"source":[""]}]}